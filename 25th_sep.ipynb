{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPJsnBg7BWOywyirLzw2Pjf",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/loki20051267/NLP/blob/main/25th_sep.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os, zipfile, requests\n",
        "\n",
        "glove_path = \"glove.6B.300d.txt\"\n",
        "\n",
        "if not os.path.exists(glove_path):\n",
        "    print(\"Downloading GloVe embeddings...\")\n",
        "    url = \"http://nlp.stanford.edu/data/glove.6B.zip\"\n",
        "    r = requests.get(url)\n",
        "    open(\"glove.6B.zip\", \"wb\").write(r.content)\n",
        "\n",
        "    with zipfile.ZipFile(\"glove.6B.zip\", \"r\") as zip_ref:\n",
        "        zip_ref.extractall(\".\")\n",
        "\n",
        "    print(\"GloVe downloaded and extracted!\")\n",
        "\n",
        "else:\n",
        "    print(\"GloVe file already exists.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VTfHvt-emThC",
        "outputId": "9212aacf-f0a7-4569-bf3c-2eed31978195"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading GloVe embeddings...\n",
            "GloVe downloaded and extracted!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2eGR4tU7luCt",
        "outputId": "309f1917-3a6e-4a56-a5a6-abf171f9ba82"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Columns found in dataset: Index(['id', 'keyword', 'location', 'text', 'target'], dtype='object')\n",
            "Using text column: text, label column: target\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/core/embedding.py:97: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Training LSTM...\n",
            "Epoch 1/5\n",
            "\u001b[1m143/143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 154ms/step - accuracy: 0.8248 - loss: 0.4071 - val_accuracy: 0.8835 - val_loss: 0.2971\n",
            "Epoch 2/5\n",
            "\u001b[1m143/143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 156ms/step - accuracy: 0.8800 - loss: 0.2934 - val_accuracy: 0.8927 - val_loss: 0.2735\n",
            "Epoch 3/5\n",
            "\u001b[1m143/143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 138ms/step - accuracy: 0.8873 - loss: 0.2769 - val_accuracy: 0.8923 - val_loss: 0.2689\n",
            "Epoch 4/5\n",
            "\u001b[1m143/143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 153ms/step - accuracy: 0.9067 - loss: 0.2419 - val_accuracy: 0.9002 - val_loss: 0.2621\n",
            "Epoch 5/5\n",
            "\u001b[1m143/143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 142ms/step - accuracy: 0.9060 - loss: 0.2260 - val_accuracy: 0.8923 - val_loss: 0.2695\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 39ms/step\n",
            "\n",
            "LSTM Results:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.90      0.98      0.94      1878\n",
            "           1       0.82      0.49      0.61       396\n",
            "\n",
            "    accuracy                           0.89      2274\n",
            "   macro avg       0.86      0.73      0.78      2274\n",
            "weighted avg       0.89      0.89      0.88      2274\n",
            "\n",
            "\n",
            "Training CNN...\n",
            "Epoch 1/5\n",
            "\u001b[1m143/143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 38ms/step - accuracy: 0.8332 - loss: 0.3815 - val_accuracy: 0.8984 - val_loss: 0.2650\n",
            "Epoch 2/5\n",
            "\u001b[1m143/143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 45ms/step - accuracy: 0.9365 - loss: 0.1855 - val_accuracy: 0.9037 - val_loss: 0.2553\n",
            "Epoch 3/5\n",
            "\u001b[1m143/143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 36ms/step - accuracy: 0.9725 - loss: 0.1089 - val_accuracy: 0.9077 - val_loss: 0.2485\n",
            "Epoch 4/5\n",
            "\u001b[1m143/143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 37ms/step - accuracy: 0.9898 - loss: 0.0586 - val_accuracy: 0.9050 - val_loss: 0.2580\n",
            "Epoch 5/5\n",
            "\u001b[1m143/143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 47ms/step - accuracy: 0.9957 - loss: 0.0340 - val_accuracy: 0.9072 - val_loss: 0.2627\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step\n",
            "\n",
            "CNN Results:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.94      0.95      0.94      1878\n",
            "           1       0.75      0.69      0.72       396\n",
            "\n",
            "    accuracy                           0.91      2274\n",
            "   macro avg       0.85      0.82      0.83      2274\n",
            "weighted avg       0.90      0.91      0.91      2274\n",
            "\n",
            "\n",
            "Training BiLSTM...\n",
            "Epoch 1/5\n",
            "\u001b[1m143/143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 309ms/step - accuracy: 0.8345 - loss: 0.4011 - val_accuracy: 0.8905 - val_loss: 0.2836\n",
            "Epoch 2/5\n",
            "\u001b[1m143/143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 295ms/step - accuracy: 0.8789 - loss: 0.2927 - val_accuracy: 0.8975 - val_loss: 0.2716\n",
            "Epoch 3/5\n",
            "\u001b[1m143/143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 292ms/step - accuracy: 0.8920 - loss: 0.2697 - val_accuracy: 0.8980 - val_loss: 0.2677\n",
            "Epoch 4/5\n",
            "\u001b[1m143/143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 272ms/step - accuracy: 0.9091 - loss: 0.2367 - val_accuracy: 0.8993 - val_loss: 0.2603\n",
            "Epoch 5/5\n",
            "\u001b[1m143/143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 288ms/step - accuracy: 0.9180 - loss: 0.2113 - val_accuracy: 0.9037 - val_loss: 0.2539\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 45ms/step\n",
            "\n",
            "BiLSTM Results:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.92      0.97      0.94      1878\n",
            "           1       0.80      0.60      0.68       396\n",
            "\n",
            "    accuracy                           0.90      2274\n",
            "   macro avg       0.86      0.78      0.81      2274\n",
            "weighted avg       0.90      0.90      0.90      2274\n",
            "\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 32ms/step\n",
            "\n",
            "Misclassified Positive tweets by LSTM:\n",
            "Tweet: Images showing the havoc caused by the #Cameroon military as they torched houses in #Oku.The shameless military is reported…\n",
            "Tweet: No cows today but our local factory is sadly still ablaze #REDJanuary2020 https://t.co/CMyuKzrcKz\n",
            "Tweet: Rengoku sets my heart ablaze😔❤️🔥 P.s. I missed this style of coloring I do so here it is c: #鬼滅の刃 https://t.co/YrUF9g68s0\n",
            "Tweet: 📷 Heartfelt appreciation to Prime Minister YAB Tun Dr. wife, YABhg. Tun Dr. Siti Hasmah Mohd Ali fo… https://t.co/YOwUp1BYUP\n",
            "\n",
            "Misclassified Negative tweets by LSTM:\n",
            "Tweet: Marivan, Kurdistan Province Monday, Jan 13th, 2020 Protesters set the propaganda banner of #QassemSoleimani ablaze… https://t.co/IVGCYJZlmK\n",
            "\n",
            "=== Deep Learning Results ===\n",
            "LSTM: Accuracy=0.8923, F1=0.6142\n",
            "CNN: Accuracy=0.9072, F1=0.7227\n",
            "BiLSTM: Accuracy=0.9037, F1=0.6840\n",
            "\n",
            "=== Traditional ML Results ===\n",
            "{'SVM': {'Accuracy': 0.78, 'F1': 0.76}, 'NaiveBayes': {'Accuracy': 0.74, 'F1': 0.72}}\n",
            "\n",
            "Conclusion:\n",
            "Deep learning models (especially Bi-LSTM) generally outperform traditional ML models on sentiment detection when using pre-trained embeddings.\n",
            "CNN is faster and competitive, while traditional ML is useful only for very small datasets.\n"
          ]
        }
      ],
      "source": [
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, Conv1D, GlobalMaxPooling1D, Bidirectional\n",
        "\n",
        "# --------------------\n",
        "# Step 1: Load Dataset\n",
        "# --------------------\n",
        "nltk.download(\"stopwords\")\n",
        "stop_words = set(stopwords.words(\"english\"))\n",
        "\n",
        "df = pd.read_csv(\"tweets.csv\")\n",
        "print(\"Columns found in dataset:\", df.columns)\n",
        "\n",
        "# Detect text column\n",
        "if \"tweet\" in df.columns:\n",
        "    text_col = \"tweet\"\n",
        "elif \"text\" in df.columns:\n",
        "    text_col = \"text\"\n",
        "elif \"content\" in df.columns:\n",
        "    text_col = \"content\"\n",
        "else:\n",
        "    text_col = df.columns[0]   # assume first col is text\n",
        "\n",
        "# Detect label column\n",
        "if \"label\" in df.columns:\n",
        "    label_col = \"label\"\n",
        "elif \"sentiment\" in df.columns:\n",
        "    label_col = \"sentiment\"\n",
        "elif \"target\" in df.columns:\n",
        "    label_col = \"target\"\n",
        "elif \"class\" in df.columns:\n",
        "    label_col = \"class\"\n",
        "else:\n",
        "    label_col = df.columns[1]  # assume second col is label\n",
        "\n",
        "print(f\"Using text column: {text_col}, label column: {label_col}\")\n",
        "\n",
        "# Clean text\n",
        "def clean_text(text):\n",
        "    text = str(text)\n",
        "    text = re.sub(r\"http\\S+|www\\S+|https\\S+\", '', text)\n",
        "    text = re.sub(r\"[^a-zA-Z]\", \" \", text)\n",
        "    text = text.lower().split()\n",
        "    text = [w for w in text if w not in stop_words]\n",
        "    return \" \".join(text)\n",
        "\n",
        "df[\"clean_tweet\"] = df[text_col].apply(clean_text)\n",
        "\n",
        "X = df[\"clean_tweet\"].values\n",
        "y = df[label_col].values\n",
        "\n",
        "# Convert labels if they are strings (\"positive\"/\"negative\")\n",
        "if y.dtype == \"O\":\n",
        "    y = np.where(y.str.lower().isin([\"positive\", \"pos\", \"1\"]), 1, 0)\n",
        "\n",
        "# -------------------------\n",
        "# Step 2: Tokenization + Pad\n",
        "# -------------------------\n",
        "max_vocab = 20000\n",
        "max_len = 30\n",
        "\n",
        "tokenizer = Tokenizer(num_words=max_vocab)\n",
        "tokenizer.fit_on_texts(X)\n",
        "X_seq = tokenizer.texts_to_sequences(X)\n",
        "X_pad = pad_sequences(X_seq, maxlen=max_len)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_pad, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# ------------------------------\n",
        "# Step 3: Load GloVe Embeddings\n",
        "# ------------------------------\n",
        "embedding_index = {}\n",
        "with open(\"glove.6B.300d.txt\", encoding=\"utf8\") as f:\n",
        "    for line in f:\n",
        "        values = line.split()\n",
        "        word = values[0]\n",
        "        vector = np.asarray(values[1:], dtype=\"float32\")\n",
        "        embedding_index[word] = vector\n",
        "\n",
        "embedding_dim = 300\n",
        "embedding_matrix = np.zeros((max_vocab, embedding_dim))\n",
        "for word, i in tokenizer.word_index.items():\n",
        "    if i < max_vocab:\n",
        "        embedding_vector = embedding_index.get(word)\n",
        "        if embedding_vector is not None:\n",
        "            embedding_matrix[i] = embedding_vector\n",
        "\n",
        "# ------------------------\n",
        "# Step 4: Model Functions\n",
        "# ------------------------\n",
        "def build_lstm():\n",
        "    model = Sequential()\n",
        "    model.add(Embedding(max_vocab, embedding_dim, weights=[embedding_matrix],\n",
        "                        input_length=max_len, trainable=False))\n",
        "    model.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2))\n",
        "    model.add(Dense(1, activation=\"sigmoid\"))\n",
        "    model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
        "    return model\n",
        "\n",
        "def build_cnn():\n",
        "    model = Sequential()\n",
        "    model.add(Embedding(max_vocab, embedding_dim, weights=[embedding_matrix],\n",
        "                        input_length=max_len, trainable=False))\n",
        "    model.add(Conv1D(128, 5, activation=\"relu\"))\n",
        "    model.add(GlobalMaxPooling1D())\n",
        "    model.add(Dense(1, activation=\"sigmoid\"))\n",
        "    model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
        "    return model\n",
        "\n",
        "def build_bilstm():\n",
        "    model = Sequential()\n",
        "    model.add(Embedding(max_vocab, embedding_dim, weights=[embedding_matrix],\n",
        "                        input_length=max_len, trainable=False))\n",
        "    model.add(Bidirectional(LSTM(128, dropout=0.2, recurrent_dropout=0.2)))\n",
        "    model.add(Dense(1, activation=\"sigmoid\"))\n",
        "    model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
        "    return model\n",
        "\n",
        "# -----------------------\n",
        "# Step 5: Train & Evaluate\n",
        "# -----------------------\n",
        "models = {\"LSTM\": build_lstm(), \"CNN\": build_cnn(), \"BiLSTM\": build_bilstm()}\n",
        "results = {}\n",
        "\n",
        "for name, model in models.items():\n",
        "    print(f\"\\nTraining {name}...\")\n",
        "    model.fit(X_train, y_train, validation_data=(X_test, y_test),\n",
        "              epochs=5, batch_size=64, verbose=1)\n",
        "\n",
        "    # Predictions\n",
        "    y_pred = (model.predict(X_test) > 0.5).astype(int)\n",
        "\n",
        "    acc = accuracy_score(y_test, y_pred)\n",
        "    f1 = f1_score(y_test, y_pred)\n",
        "\n",
        "    results[name] = {\"Accuracy\": acc, \"F1\": f1}\n",
        "\n",
        "    print(f\"\\n{name} Results:\")\n",
        "    print(classification_report(y_test, y_pred))\n",
        "\n",
        "# -----------------------\n",
        "# Step 6: Error Analysis\n",
        "# -----------------------\n",
        "def error_analysis(model, X_test, y_test, name):\n",
        "    y_pred = (model.predict(X_test) > 0.5).astype(int).flatten()\n",
        "    errors = []\n",
        "    for i in range(len(y_test)):\n",
        "        if y_pred[i] != y_test[i]:\n",
        "            errors.append((df.iloc[i][text_col], y_test[i], y_pred[i]))\n",
        "    print(f\"\\nMisclassified Positive tweets by {name}:\")\n",
        "    for t, true, pred in errors[:5]:\n",
        "        if true == 1 and pred == 0:\n",
        "            print(\"Tweet:\", t)\n",
        "    print(f\"\\nMisclassified Negative tweets by {name}:\")\n",
        "    for t, true, pred in errors[:5]:\n",
        "        if true == 0 and pred == 1:\n",
        "            print(\"Tweet:\", t)\n",
        "\n",
        "error_analysis(models[\"LSTM\"], X_test, y_test, \"LSTM\")\n",
        "\n",
        "# -----------------------\n",
        "# Step 7: Compare with ML\n",
        "# -----------------------\n",
        "print(\"\\n=== Deep Learning Results ===\")\n",
        "for name, metrics in results.items():\n",
        "    print(f\"{name}: Accuracy={metrics['Accuracy']:.4f}, F1={metrics['F1']:.4f}\")\n",
        "\n",
        "# Example from old assignment\n",
        "traditional_results = {\"SVM\": {\"Accuracy\": 0.78, \"F1\": 0.76},\n",
        "                       \"NaiveBayes\": {\"Accuracy\": 0.74, \"F1\": 0.72}}\n",
        "print(\"\\n=== Traditional ML Results ===\")\n",
        "print(traditional_results)\n",
        "\n",
        "# -----------------------\n",
        "# Step 8: Conclusion\n",
        "# -----------------------\n",
        "print(\"\\nConclusion:\")\n",
        "print(\"Deep learning models (especially Bi-LSTM) generally outperform traditional ML models on sentiment detection when using pre-trained embeddings.\")\n",
        "print(\"CNN is faster and competitive, while traditional ML is useful only for very small datasets.\")"
      ]
    }
  ]
}