{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPaDqhsP8GOmWzANmm7RKEM",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/loki20051267/NLP/blob/main/12th_sep.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kynpxvg8S3RS",
        "outputId": "ba185558-17e2-4cc4-e137-4eb8a780bd36"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   id keyword        location  \\\n",
            "0   0  ablaze             NaN   \n",
            "1   1  ablaze             NaN   \n",
            "2   2  ablaze   New York City   \n",
            "3   3  ablaze  Morgantown, WV   \n",
            "4   4  ablaze             NaN   \n",
            "\n",
            "                                                text  target  \n",
            "0  Communal violence in Bhainsa, Telangana. \"Ston...       1  \n",
            "1  Telangana: Section 144 has been imposed in Bha...       1  \n",
            "2  Arsonist sets cars ablaze at dealership https:...       1  \n",
            "3  Arsonist sets cars ablaze at dealership https:...       1  \n",
            "4  \"Lord Jesus, your love brings freedom and pard...       0  \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Logistic Regression (TF-IDF) Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.8804    0.9920    0.9329      1878\n",
            "           1     0.9051    0.3611    0.5162       396\n",
            "\n",
            "    accuracy                         0.8821      2274\n",
            "   macro avg     0.8927    0.6766    0.7246      2274\n",
            "weighted avg     0.8847    0.8821    0.8603      2274\n",
            "\n",
            "\n",
            "SVM (TF-IDF) Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.8933    0.9941    0.9410      1878\n",
            "           1     0.9402    0.4369    0.5966       396\n",
            "\n",
            "    accuracy                         0.8971      2274\n",
            "   macro avg     0.9168    0.7155    0.7688      2274\n",
            "weighted avg     0.9015    0.8971    0.8810      2274\n",
            "\n",
            "Epoch 1/5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/core/embedding.py:97: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m228/228\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 13ms/step - accuracy: 0.8039 - loss: 0.4748 - val_accuracy: 0.8863 - val_loss: 0.3242\n",
            "Epoch 2/5\n",
            "\u001b[1m228/228\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 14ms/step - accuracy: 0.9190 - loss: 0.2170 - val_accuracy: 0.8885 - val_loss: 0.3234\n",
            "Epoch 3/5\n",
            "\u001b[1m228/228\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 14ms/step - accuracy: 0.9772 - loss: 0.1144 - val_accuracy: 0.8857 - val_loss: 0.3915\n",
            "Epoch 4/5\n",
            "\u001b[1m228/228\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 15ms/step - accuracy: 0.9926 - loss: 0.0408 - val_accuracy: 0.8802 - val_loss: 0.4619\n",
            "Epoch 5/5\n",
            "\u001b[1m228/228\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 14ms/step - accuracy: 0.9958 - loss: 0.0205 - val_accuracy: 0.8654 - val_loss: 0.5014\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
            "\n",
            "MLP (Embeddings) Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.9356    0.9207    0.9281      1878\n",
            "           1     0.6502    0.6995    0.6740       396\n",
            "\n",
            "    accuracy                         0.8821      2274\n",
            "   macro avg     0.7929    0.8101    0.8010      2274\n",
            "weighted avg     0.8859    0.8821    0.8838      2274\n",
            "\n",
            "Epoch 1/5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/core/embedding.py:97: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m228/228\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 21ms/step - accuracy: 0.7941 - loss: 0.4880 - val_accuracy: 0.8863 - val_loss: 0.3051\n",
            "Epoch 2/5\n",
            "\u001b[1m228/228\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 25ms/step - accuracy: 0.9355 - loss: 0.1936 - val_accuracy: 0.8901 - val_loss: 0.3148\n",
            "Epoch 3/5\n",
            "\u001b[1m228/228\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 21ms/step - accuracy: 0.9795 - loss: 0.0755 - val_accuracy: 0.8709 - val_loss: 0.3927\n",
            "Epoch 4/5\n",
            "\u001b[1m228/228\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 24ms/step - accuracy: 0.9924 - loss: 0.0294 - val_accuracy: 0.8747 - val_loss: 0.5220\n",
            "Epoch 5/5\n",
            "\u001b[1m228/228\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 22ms/step - accuracy: 0.9943 - loss: 0.0194 - val_accuracy: 0.8736 - val_loss: 0.5395\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
            "\n",
            "CNN (Embeddings) Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.9232    0.9286    0.9259      1878\n",
            "           1     0.6519    0.6338    0.6428       396\n",
            "\n",
            "    accuracy                         0.8773      2274\n",
            "   macro avg     0.7876    0.7812    0.7844      2274\n",
            "weighted avg     0.8760    0.8773    0.8766      2274\n",
            "\n",
            "Epoch 1/5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/core/embedding.py:97: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m228/228\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 140ms/step - accuracy: 0.8083 - loss: 0.5064 - val_accuracy: 0.8121 - val_loss: 0.4849\n",
            "Epoch 2/5\n",
            "\u001b[1m228/228\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 117ms/step - accuracy: 0.8088 - loss: 0.4901 - val_accuracy: 0.8121 - val_loss: 0.4836\n",
            "Epoch 3/5\n",
            "\u001b[1m228/228\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 117ms/step - accuracy: 0.8103 - loss: 0.4869 - val_accuracy: 0.8121 - val_loss: 0.4869\n",
            "Epoch 4/5\n",
            "\u001b[1m228/228\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 118ms/step - accuracy: 0.8143 - loss: 0.4810 - val_accuracy: 0.8121 - val_loss: 0.4853\n",
            "Epoch 5/5\n",
            "\u001b[1m228/228\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 124ms/step - accuracy: 0.8129 - loss: 0.4838 - val_accuracy: 0.8121 - val_loss: 0.4833\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 21ms/step\n",
            "\n",
            "LSTM (Embeddings) Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.8259    1.0000    0.9046      1878\n",
            "           1     0.0000    0.0000    0.0000       396\n",
            "\n",
            "    accuracy                         0.8259      2274\n",
            "   macro avg     0.4129    0.5000    0.4523      2274\n",
            "weighted avg     0.6820    0.8259    0.7471      2274\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import re\n",
        "import string\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report\n",
        "\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, Dense, Flatten, Conv1D, GlobalMaxPooling1D, LSTM, Dropout\n",
        "\n",
        "\n",
        "df = pd.read_csv(\"tweets.csv\")\n",
        "print(df.head())\n",
        "\n",
        "\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "def clean_text(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'@[A-Za-z0-9_]+','', text)\n",
        "    text = re.sub(r'#','', text)\n",
        "    text = re.sub(r'http\\S+|www.\\S+','', text)\n",
        "    text = re.sub(r'[^a-z\\s]', '', text)\n",
        "    tokens = text.split()\n",
        "    tokens = [w for w in tokens if w not in stop_words]\n",
        "    return \" \".join(tokens)\n",
        "\n",
        "df['clean'] = df['text'].apply(clean_text)\n",
        "\n",
        "X = df['clean']\n",
        "y = df['target']\n",
        "\n",
        "\n",
        "count_vectorizer = CountVectorizer()\n",
        "X_count = count_vectorizer.fit_transform(X)\n",
        "\n",
        "tfidf_vectorizer = TfidfVectorizer()\n",
        "X_tfidf = tfidf_vectorizer.fit_transform(X)\n",
        "\n",
        "X_train_tfidf, X_test_tfidf, y_train, y_test = train_test_split(X_tfidf, y, test_size=0.2, random_state=42)\n",
        "\n",
        "\n",
        "log_reg = LogisticRegression(max_iter=200)\n",
        "log_reg.fit(X_train_tfidf, y_train)\n",
        "y_pred_lr = log_reg.predict(X_test_tfidf)\n",
        "\n",
        "svm = SVC()\n",
        "svm.fit(X_train_tfidf, y_train)\n",
        "y_pred_svm = svm.predict(X_test_tfidf)\n",
        "\n",
        "def evaluate_model(y_true, y_pred, name=\"Model\"):\n",
        "    print(f\"\\n{name} Report:\")\n",
        "    print(classification_report(y_true, y_pred, digits=4))\n",
        "\n",
        "evaluate_model(y_test, y_pred_lr, \"Logistic Regression (TF-IDF)\")\n",
        "evaluate_model(y_test, y_pred_svm, \"SVM (TF-IDF)\")\n",
        "\n",
        "# =========================\n",
        "# 6. WORD EMBEDDINGS (Tokenizer + Padding)\n",
        "# =========================\n",
        "max_words = 10000\n",
        "max_len = 50\n",
        "embedding_dim = 100\n",
        "\n",
        "tokenizer = Tokenizer(num_words=max_words, oov_token=\"<OOV>\")\n",
        "tokenizer.fit_on_texts(X)\n",
        "sequences = tokenizer.texts_to_sequences(X)\n",
        "X_pad = pad_sequences(sequences, maxlen=max_len, padding='post')\n",
        "\n",
        "X_train_pad, X_test_pad, y_train_pad, y_test_pad = train_test_split(X_pad, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# =========================\n",
        "# 7. DEEP LEARNING MODELS\n",
        "# =========================\n",
        "\n",
        "# ---- (a) MLP on averaged embeddings ----\n",
        "mlp = Sequential([\n",
        "    Embedding(max_words, embedding_dim, input_length=max_len),\n",
        "    Flatten(),\n",
        "    Dense(64, activation='relu'),\n",
        "    Dropout(0.5),\n",
        "    Dense(1, activation='sigmoid')\n",
        "])\n",
        "mlp.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "mlp.fit(X_train_pad, y_train_pad, epochs=5, batch_size=32, validation_split=0.2, verbose=1)\n",
        "mlp_pred = (mlp.predict(X_test_pad) > 0.5).astype(int)\n",
        "evaluate_model(y_test_pad, mlp_pred, \"MLP (Embeddings)\")\n",
        "\n",
        "# ---- (b) 1D CNN ----\n",
        "cnn = Sequential([\n",
        "    Embedding(max_words, embedding_dim, input_length=max_len),\n",
        "    Conv1D(128, 5, activation='relu'),\n",
        "    GlobalMaxPooling1D(),\n",
        "    Dense(64, activation='relu'),\n",
        "    Dropout(0.5),\n",
        "    Dense(1, activation='sigmoid')\n",
        "])\n",
        "cnn.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "cnn.fit(X_train_pad, y_train_pad, epochs=5, batch_size=32, validation_split=0.2, verbose=1)\n",
        "cnn_pred = (cnn.predict(X_test_pad) > 0.5).astype(int)\n",
        "evaluate_model(y_test_pad, cnn_pred, \"CNN (Embeddings)\")\n",
        "\n",
        "# ---- (c) LSTM ----\n",
        "lstm = Sequential([\n",
        "    Embedding(max_words, embedding_dim, input_length=max_len),\n",
        "    LSTM(128, dropout=0.2, recurrent_dropout=0.2),\n",
        "    Dense(1, activation='sigmoid')\n",
        "])\n",
        "lstm.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "lstm.fit(X_train_pad, y_train_pad, epochs=5, batch_size=32, validation_split=0.2, verbose=1)\n",
        "lstm_pred = (lstm.predict(X_test_pad) > 0.5).astype(int)\n",
        "evaluate_model(y_test_pad, lstm_pred, \"LSTM (Embeddings)\")\n"
      ]
    }
  ]
}